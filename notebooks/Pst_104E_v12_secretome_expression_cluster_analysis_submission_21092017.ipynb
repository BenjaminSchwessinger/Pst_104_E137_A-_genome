{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to look at the expression pattern of the different secretome expression clusters shown in Figure 4 of the paper. It aims to determine the following.\n",
    "* How many genes of cluster X are alleles, non-allelic inter-haplome paralogs (non-allelic protein 'orthoglos'), and how many are singletons\n",
    "* How many alleles of cluster X in haplotype Y are expressed in cluster Z in halpotype Y and vice versa\n",
    "\n",
    "The input data is the secretome expression cluster data provided by Jana Speerschneider and the allele analysis done by Benjamin Schwessinger\n",
    "\n",
    "This notebook was only designed for the purpose of analyzing the Pst-104E genome. No gurantees it works in any other situtation. It will have spelling errors due to the lack of autocorrection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/Bio/SearchIO/__init__.py:211: BiopythonExperimentalWarning: Bio.SearchIO is an experimental submodule which may undergo significant changes prior to its future official release.\n",
      "  BiopythonExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "from Bio import SeqUtils\n",
    "import pysam\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import pybedtools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "from Bio.Seq import Seq\n",
    "import pysam\n",
    "from Bio import SearchIO\n",
    "import json\n",
    "import glob\n",
    "import scipy.stats as stats\n",
    "import statsmodels as sms\n",
    "import statsmodels.sandbox.stats.multicomp\n",
    "import distance\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define some PATH\n",
    "BASE_AA_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12'\n",
    "POST_ALLELE_ANALYSIS_PATH = os.path.join(BASE_AA_PATH, 'post_allele_analysis', \\\n",
    "                'proteinortho_graph516_QC_Qcov80_PctID70_evalue01')\n",
    "OUT_PATH = os.path.join(POST_ALLELE_ANALYSIS_PATH , \\\n",
    "                       'secretome_expression_clusters')\n",
    "CLUSTER_PATH_P = os.path.join(BASE_AA_PATH, 'Pst_104E_genome',\\\n",
    "                              'gene_expression', 'Pst104_p_SecretomeClustering' )\n",
    "CLUSTER_PATH_H = os.path.join(BASE_AA_PATH, 'Pst_104E_genome',\\\n",
    "                              'gene_expression', 'Pst104_h_SecretomeClustering' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some list to order the output later on\n",
    "haplotig_cluster_order = ['Cluster9', 'Cluster10', 'Cluster11', 'Cluster12', 'Cluster13', 'Cluster14',\\\n",
    "        'Cluster15', 'Cluster16']\n",
    "primary_cluster_order = ['Cluster1', 'Cluster2', 'Cluster3', 'Cluster4', 'Cluster5', 'Cluster6',\\\n",
    "        'Cluster7', 'Cluster8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the different classes of genes e.g. alleles, non-allelic protein 'orthoglos',  \\\n",
    "#loose_singletons (still including unphased genes), singletons\n",
    "allele_fn = 'Pst_104E_v12_p_ctg.all.alleles'\n",
    "loose_singletons_fn = 'Pst_104E_v12_ph_ctg.loose_singletons'\n",
    "singletons_fn = 'Pst_104E_v12_ph_ctg.singletons'\n",
    "nap_fn = 'Pst_104E_v12_ph_ctg.no_alleles_orthologs'\n",
    "\n",
    "alleles_df = pd.read_csv(os.path.join(POST_ALLELE_ANALYSIS_PATH, allele_fn), header=None,\\\n",
    "                        sep='\\t', names=['p_genes', 'h_genes'])\n",
    "loose_sing_array = pd.read_csv(os.path.join(POST_ALLELE_ANALYSIS_PATH, loose_singletons_fn),\n",
    "                             header=None, sep='\\t')[0]\n",
    "sing_array = pd.read_csv(os.path.join(POST_ALLELE_ANALYSIS_PATH, singletons_fn),\n",
    "                             header=None, sep='\\t')[0]\n",
    "nap_array = pd.read_csv(os.path.join(POST_ALLELE_ANALYSIS_PATH, nap_fn),\n",
    "                             header=None, sep='\\t')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now get the different gene clusters in a df with the following set up\n",
    "#columns = gene, cluster, allele status, allele_ID\n",
    "primary_df = pd.DataFrame(columns=['gene', 'cluster_ID', 'allele_state', 'allele_ID'])\n",
    "haplotig_df = pd.DataFrame(columns=['gene', 'cluster_ID', 'allele_state', 'allele_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the genes and the cluster ID as fn in list of equal lenght to be used as gene and \n",
    "#cluster_ID columns\n",
    "_gene_list = []\n",
    "_cluster_list = []\n",
    "for file in [x for x in os.listdir(CLUSTER_PATH_P) if x.endswith('_DEs.fasta')]:\n",
    "    for seq in SeqIO.parse(open(os.path.join(CLUSTER_PATH_P,file), 'r'), 'fasta'):\n",
    "        _gene_list.append(seq.id)\n",
    "        _cluster_list.append(file.split('_')[0])\n",
    "primary_df.gene = _gene_list\n",
    "primary_df.cluster_ID = _cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now populate the allele_state list by setting the value in the allele_state column\n",
    "#nomenclatures are alleles, nap, loose_singletons (unphased singletons), singletons (True singletons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_df.loc[\\\n",
    "               primary_df[primary_df.gene.isin(alleles_df.p_genes)].index,\\\n",
    "               'allele_state'] = \"allelic\"\n",
    "primary_df.loc[\\\n",
    "               primary_df[primary_df.gene.isin(sing_array)].index,\\\n",
    "               'allele_state'] = 'singleton'\n",
    "primary_df.loc[\\\n",
    "               primary_df[primary_df.gene.isin(nap_array)].index,\\\n",
    "               'allele_state'] = 'nap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now do the same thing for the haplotig sequences\n",
    "#get the genes and the cluster ID as fn in list of equal lenght to be used as gene and \n",
    "#cluster_ID columns\n",
    "_gene_list = []\n",
    "_cluster_list = []\n",
    "for file in [x for x in os.listdir(CLUSTER_PATH_H) if x.endswith('_DEs.fasta')]:\n",
    "    for seq in SeqIO.parse(open(os.path.join(CLUSTER_PATH_H,file), 'r'), 'fasta'):\n",
    "        _gene_list.append(seq.id)\n",
    "        _cluster_list.append(file.split('_')[0])\n",
    "haplotig_df.gene = _gene_list\n",
    "haplotig_df.cluster_ID = _cluster_list\n",
    "haplotig_df.loc[\\\n",
    "               haplotig_df[haplotig_df.gene.isin(alleles_df.h_genes)].index,\\\n",
    "               'allele_state'] = \"allelic\"\n",
    "haplotig_df.loc[\\\n",
    "               haplotig_df[haplotig_df.gene.isin(loose_sing_array)].index,\\\n",
    "               'allele_state'] = 'singleton'\n",
    "haplotig_df.loc[\\\n",
    "               haplotig_df[haplotig_df.gene.isin(nap_array)].index,\\\n",
    "               'allele_state'] = 'nap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now summarize the allele states and write them out to file\n",
    "#first aggregateon cluster_ID and allele_state + unstack\n",
    "primary_allele_state_df = primary_df.loc[:,['gene','cluster_ID','allele_state']]\\\n",
    ".pivot_table(columns=['cluster_ID','allele_state'],aggfunc='count').unstack()\n",
    "#drop the unneccessary gene level from the index and replace na with 0\n",
    "primary_allele_state_df.index = primary_allele_state_df.index.droplevel()\n",
    "primary_allele_state_df.fillna(0)\n",
    "#add a total number as well\n",
    "primary_allele_state_df['Total'] = primary_allele_state_df.sum(axis=1)\n",
    "#save dataframe\n",
    "out_fn = 'Pst_104E_v12_p_ctg.cluster_status_summary.df'\n",
    "primary_allele_state_df.fillna(0).T.loc[:,\\\n",
    "        ['Cluster1', 'Cluster2', 'Cluster3', 'Cluster4', 'Cluster5', 'Cluster6',\\\n",
    "        'Cluster7', 'Cluster8']].to_csv(os.path.join(OUT_PATH, out_fn), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now summarize the allele states and write them out to file\n",
    "#first aggregateon cluster_ID and allele_state + unstack\n",
    "haplotig_allele_state_df = haplotig_df.loc[:,['gene','cluster_ID','allele_state']]\\\n",
    ".pivot_table(columns=['cluster_ID','allele_state'],aggfunc='count').unstack()\n",
    "#drop the unneccessary gene level from the index and replace na with 0\n",
    "haplotig_allele_state_df.index = haplotig_allele_state_df.index.droplevel()\n",
    "haplotig_allele_state_df.fillna(0)\n",
    "#add a total number as well\n",
    "haplotig_allele_state_df['Total'] = haplotig_allele_state_df.sum(axis=1)\n",
    "#save dataframe\n",
    "out_fn = 'Pst_104E_v12_h_ctg.cluster_status_summary.df'\n",
    "haplotig_allele_state_df.fillna(0).T.loc[:,\\\n",
    "        ['Cluster9', 'Cluster10', 'Cluster11', 'Cluster12', 'Cluster13', 'Cluster14',\\\n",
    "        'Cluster15', 'Cluster16']].to_csv(os.path.join(OUT_PATH, out_fn), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#get the allele for each gene using a dict approach that also takes care of potential multiple\n",
    "# alleles\n",
    "allele_single_dict = {}\n",
    "allele_multiple_dict = {}\n",
    "#take all the allelic genes and pick the corresponding allele form the allele_df\n",
    "#if there are multiple possible allele pairings add those as list to a different dictionary\n",
    "for gene in primary_df[primary_df.allele_state == 'allelic'].gene:\n",
    "    if len(alleles_df[alleles_df.p_genes == gene].h_genes.tolist()) == 1:\n",
    "        allele_single_dict[gene] = alleles_df[alleles_df.p_genes == gene].h_genes.tolist()[0]\n",
    "    elif len(alleles_df[alleles_df.p_genes == gene].h_genes.tolist()) != 1:\n",
    "        print(len(alleles_df[alleles_df.p_genes == gene].h_genes.tolist()))\n",
    "        allele_multiple_dict[gene] = alleles_df[alleles_df.p_genes == gene].h_genes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for gene in haplotig_df[haplotig_df.allele_state == 'allelic'].gene:\n",
    "    if len(alleles_df[alleles_df.h_genes == gene].p_genes.tolist()) == 1:\n",
    "        allele_single_dict[gene] = alleles_df[alleles_df.h_genes == gene].p_genes.tolist()[0]\n",
    "    elif len(alleles_df[alleles_df.h_genes == gene].p_genes.tolist()) != 1:\n",
    "        print(len(alleles_df[alleles_df.h_genes == gene].p_genes.tolist()))\n",
    "        allele_multiple_dict[gene] = alleles_df[alleles_df.h_genes == gene].p_genes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#frist add the single allele pairing to the dataframes\n",
    "def add_single_alleles(x, _dict1=allele_single_dict,_dict2=allele_multiple_dict):\n",
    "    if x in _dict1.keys():\n",
    "        return _dict1[x]\n",
    "    elif x in _dict2:\n",
    "        return 'multiples'\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_df.allele_ID = primary_df.gene.apply(add_single_alleles)\n",
    "haplotig_df.allele_ID = haplotig_df.gene.apply(add_single_alleles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now take care of the genes that have multiple alleles. In our case the biggest possible number\n",
    "#is two AND all are two so this hack\n",
    "\n",
    "#make two copies of the df that are multiples\n",
    "tmp0_df = primary_df[primary_df.allele_ID == 'multiples'].copy()\n",
    "tmp1_df = primary_df[primary_df.allele_ID == 'multiples'].copy()\n",
    "drop_index = primary_df[primary_df.allele_ID == 'multiples'].index\n",
    "#add the genes ideas to each of the copies once taking the first element and the other time\n",
    "#the second\n",
    "tmp0_df.allele_ID = tmp0_df.gene.apply(lambda x: allele_multiple_dict[x][0])\n",
    "tmp1_df.allele_ID = tmp1_df.gene.apply(lambda x: allele_multiple_dict[x][1])\n",
    "#now concat both tmp dataframes to the original dataframe while not including them in the\n",
    "#former\n",
    "primary_wa_df = pd.concat([primary_df.drop(primary_df.index[drop_index]), tmp0_df, tmp1_df], axis = 0)\n",
    "primary_wa_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now take care of the genes that have multiple alleles. In our case the biggest possible number\n",
    "#is two AND all are two so this hack\n",
    "\n",
    "#make two copies of the df that are multiples\n",
    "tmp0_df = haplotig_df[haplotig_df.allele_ID == 'multiples'].copy()\n",
    "tmp1_df = haplotig_df[haplotig_df.allele_ID == 'multiples'].copy()\n",
    "drop_index = haplotig_df[haplotig_df.allele_ID == 'multiples'].index\n",
    "#add the genes ideas to each of the copies once taking the first element and the other time\n",
    "#the second\n",
    "tmp0_df.allele_ID = tmp0_df.gene.apply(lambda x: allele_multiple_dict[x][0])\n",
    "tmp1_df.allele_ID = tmp1_df.gene.apply(lambda x: allele_multiple_dict[x][1])\n",
    "#now concat both tmp dataframes to the original dataframe while not including them in the\n",
    "#former\n",
    "haplotig_wa_df = pd.concat([haplotig_df.drop(haplotig_df.index[drop_index]), tmp0_df, tmp1_df], axis = 0)\n",
    "haplotig_wa_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py:2746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#now summaries the respective cluster hits for primary contigs\n",
    "count_list = []\n",
    "percentage_list = []\n",
    "for cluster in primary_df.cluster_ID.unique():\n",
    "    c_genes = ''\n",
    "    #subset the dataframe to get the allelic genes in each cluster\n",
    "    c_genes = primary_df[(primary_df.cluster_ID == cluster) \\\n",
    "                         & (primary_df.allele_state == 'allelic')].gene\n",
    "    #use this list to subset the other dataframe\n",
    "    _tmp_df = haplotig_wa_df[haplotig_wa_df.allele_ID.isin(c_genes)]\n",
    "    _tmp_df.rename(columns={'gene': cluster}, inplace=True)\n",
    "    #count occurances and add them to the list to make a dataframe alter\n",
    "    count_list.append(_tmp_df.groupby('cluster_ID').count()[cluster])\n",
    "    #now take care of percentage by making a count dataframe \n",
    "    _tmp_count_df = _tmp_df.groupby('cluster_ID').count().copy()\n",
    "    #and dividing series by the clusters total\n",
    "    _tmp_count_df[cluster] = _tmp_count_df[cluster].\\\n",
    "        apply(lambda x: x/primary_allele_state_df.loc[cluster, \"allelic\"]*100)\n",
    "    percentage_list.append(_tmp_count_df[cluster])\n",
    "\n",
    "#now generate some summary df by concaonating the list and adding a Total line at     \n",
    "c_out_fn = 'Pst_104E_v12_p_ctg.relatvie_cluster_allele_status_count_summary.df'\n",
    "count_df = pd.concat(count_list, axis=1)\n",
    "count_df.loc['Total',:]= count_df.sum(axis=0)\n",
    "count_df.fillna(0, inplace=True)\n",
    "count_df.astype(int).loc[haplotig_cluster_order+[\"Total\"], primary_cluster_order]\\\n",
    "    .to_csv(os.path.join(OUT_PATH, c_out_fn), sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "p_out_fn = 'Pst_104E_v12_p_ctg.relatvie_cluster_allele_status_per_summary.df'\n",
    "percentage_df = pd.concat(percentage_list, axis=1)\n",
    "percentage_df.loc['Total',:]= percentage_df.sum(axis=0)\n",
    "percentage_df.fillna(0, inplace=True)\n",
    "percentage_df.round(1).loc[haplotig_cluster_order+[\"Total\"], primary_cluster_order]\\\n",
    "    .to_csv(os.path.join(OUT_PATH, p_out_fn), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py:2746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#now summaries the respective cluster hits for haplotigs\n",
    "count_list = []\n",
    "percentage_list = []\n",
    "for cluster in haplotig_df.cluster_ID.unique():\n",
    "    c_genes = ''\n",
    "    #subset the dataframe to get the allelic genes in each cluster\n",
    "    c_genes = haplotig_df[(haplotig_df.cluster_ID == cluster) \\\n",
    "                         & (haplotig_df.allele_state == 'allelic')].gene\n",
    "    #use this list to subset the other dataframe\n",
    "    _tmp_df = primary_wa_df[primary_wa_df.allele_ID.isin(c_genes)]\n",
    "    _tmp_df.rename(columns={'gene': cluster}, inplace=True)\n",
    "    #count occurances and add them to the list to make a dataframe alter\n",
    "    count_list.append(_tmp_df.groupby('cluster_ID').count()[cluster])\n",
    "    #now take care of percentage by making a count dataframe \n",
    "    _tmp_count_df = _tmp_df.groupby('cluster_ID').count().copy()\n",
    "    #and dividing series by the clusters total\n",
    "    _tmp_count_df[cluster] = _tmp_count_df[cluster].\\\n",
    "        apply(lambda x: x/haplotig_allele_state_df.loc[cluster, \"allelic\"]*100)\n",
    "    percentage_list.append(_tmp_count_df[cluster])\n",
    "\n",
    "#now generate some summary df by concaonating the list and adding a Total line at     \n",
    "c_out_fn = 'Pst_104E_v12_h_ctg.relatvie_cluster_allele_status_count_summary.df'\n",
    "count_df = pd.concat(count_list, axis=1)\n",
    "count_df.loc['Total',:]= count_df.sum(axis=0)\n",
    "count_df.fillna(0, inplace=True)\n",
    "count_df.astype(int).loc[primary_cluster_order+[\"Total\"], haplotig_cluster_order]\\\n",
    "    .to_csv(os.path.join(OUT_PATH, c_out_fn), sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "p_out_fn = 'Pst_104E_v12_h_ctg.relatvie_cluster_allele_status_per_summary.df'\n",
    "percentage_df = pd.concat(percentage_list, axis=1)\n",
    "percentage_df.loc['Total',:]= percentage_df.sum(axis=0)\n",
    "percentage_df.fillna(0, inplace=True)\n",
    "percentage_df.round(1).loc[primary_cluster_order+[\"Total\"], haplotig_cluster_order]\\\n",
    "    .to_csv(os.path.join(OUT_PATH, p_out_fn), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#at the end fix up the allele summary dataframe for primary allele state analysis\n",
    "#at this point we count the non-phased singletons to the alleles as well in the primary\n",
    "#but leave them out initially for the relative analysis\n",
    "reset_index = primary_df[(primary_df.allele_state != 'allelic')&(primary_df.allele_state != 'nap')\\\n",
    "          &(primary_df.allele_state != 'singleton')].index\n",
    "primary_df.loc[reset_index, 'allele_state'] = 'allelic'\n",
    "#save dataframe\n",
    "#now summarize the allele states and write them out to file\n",
    "#first aggregateon cluster_ID and allele_state + unstack\n",
    "primary_allele_state_df = primary_df.loc[:,['gene','cluster_ID','allele_state']]\\\n",
    ".pivot_table(columns=['cluster_ID','allele_state'],aggfunc='count').unstack()\n",
    "#drop the unneccessary gene level from the index and replace na with 0\n",
    "primary_allele_state_df.index = primary_allele_state_df.index.droplevel()\n",
    "primary_allele_state_df.fillna(0)\n",
    "#add a total number as well\n",
    "primary_allele_state_df['Total'] = primary_allele_state_df.sum(axis=1)\n",
    "\n",
    "out_fn = 'Pst_104E_v12_p_ctg.cluster_status_summary.df'\n",
    "primary_allele_state_df.fillna(0).T.loc[:,\\\n",
    "        ['Cluster1', 'Cluster2', 'Cluster3', 'Cluster4', 'Cluster5', 'Cluster6',\\\n",
    "        'Cluster7', 'Cluster8']].to_csv(os.path.join(OUT_PATH, out_fn), sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
