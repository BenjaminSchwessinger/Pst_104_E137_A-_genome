{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed at doing quality control on the allele. This was the basis to generated dataframes used during the analysis and evolved during the analysis. So it might be a bit more complicated as it needed to be but it worked. It also contains a couple of additional thoughts for QC of an allelic analysis that didn't make it into the paper. Some of those weren't as good as others.\n",
    "\n",
    "\n",
    "This notebook was only designed for the purpose of analyzing the Pst-104E genome. No gurantees it works in any other situtation. It will have spelling errors due to the lack of autocorrection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Jupyter notebook is targeted towards QC analysis of gene models without alleles in a phased assembly\n",
    "\n",
    "The script will pick up were *_defining_alleles_v01.ipynb left off. In general it will ask the quesion why a certain gene model was not found in the other haplotig. Primary contigs p_ctg are the main genome and haplotigs are the alternative space. This is based on FALCON_UNZIP assemblies for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial allele analysis was done on the protein level. Now we wonder why an allele might be missing from the other haplotigs. For this we consider some easy explanations.\n",
    "\n",
    "* The gene annotation in one of the haplotypes went 'wrong' and the gene was simply ommited. <img src=\"pics/Number_1_missing_allele_QC_v01.png\" width=\"600\" height=\"600\">\n",
    "* The gene is situated on a primary contig and falls within a homozygous coverage region when mapping short reads against primary contigs and haplotigs. E.g. not phased in the initial assembly because of absence of SNPs. <img src=\"pics/Number_2_missing_allele_QC_v01.png\" width=\"600\" height=\"600\">\n",
    "* The gene is situated on a primary contig and falls within a homozygous coverage region when mapping short reads against primary contigs and haplotigs. E.g. not phased in the initial assembly yes has SNPs. <img src=\"pics/Number_3_missing_allele_QC_v01.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "#### 1 The allele was left out of the annotation in the other haplotig: \n",
    "* It takes the gene sequence of a single allele gene and blast its against the other haplotic\n",
    "* If there is an gene sequence it pulls out a region around this and align the protein sequence using exonerate\n",
    "* The exonerate alignment is used to scan for matches without frame shifts and without stop codons\n",
    "* In cases where a good aligment is possible these alleles are written add to the summary dataframe \n",
    "\n",
    "#### 2 and 3 The allele was left out because it was not phased in the first place (only for proteins from primary assembly):\n",
    "This step relies on the Pst_104E_v12_coverage_analysis_training script to idententify homozygous regions when mapping against primary contigs and haplotigs.\n",
    "* All remaining single allele genes are tested if they fall into a homozygous coverage (1x coverage) area during read mapping against primary contigs and haplotigs.\n",
    "* These 1x coverage regions in primary contigs should have no corresponding haplotigs. The script also annotates overlaps per gene as True/False\n",
    "* These genes in homozygous coverage regions could be unphased because of the lack of SNPs (#2) or unphased even though they have SNPs. Indeed the script looks for SNPs in all no_allele genes.\n",
    "\n",
    "#### Summary dataframe as final output:\n",
    "The final output of the script is a dataframe based on the protein blast hits. These initial blast hit dataframes from *_defining_alleles_v01.ipynb are read in from the outfmt6  \n",
    "\n",
    "e.g. Pst_104E_v12_ph_ctg.no_alleles_QC.Qcov80.PctID70.df \n",
    "\n",
    "Qcov80.PctID70  are the cut off used previously.\n",
    "\n",
    "Query   Target  PctID   AlnLgth NumMis  NumGap  StartQuery      StopQuery       StartTarget     StopTarget  e-value BitScore\n",
    "\n",
    "**And following columns added:**\n",
    "\n",
    "**Based on outfmt6 dataframe**\n",
    "\n",
    "QLgth == Query length\n",
    "\n",
    "QCov == Query Coverage    \n",
    "\n",
    "q_contig == ID of query contig (assumes pcontig_XXX or hcontig_XXX_XXX annotation)\n",
    "\n",
    "t_contig == ID of target contig (assumes pcontig_XXX or hcontig_XXX_XXX annotation) \n",
    "q_contig == t_contig -> True/False\n",
    "\n",
    "primary_contig -> True/False\n",
    "\n",
    "pwh_contig -> True/False if primary contig has at least one haplotig contig\n",
    "\n",
    "**Based on analysis in this script**\n",
    "\n",
    "gene_on_genome_blast_hit -> True/False if gene vs. genome blast hit is existent\n",
    "\n",
    "exn_asso_contig -> True/False/Nan if exonerate could align protein sequence to gene hit nt extract on associated\n",
    "\n",
    "haplotig aka q_contig == t_contig == True\n",
    "\n",
    "exn_no_asso_contig -> True/False/Nan if exonerate could align protein sequence to gene hit nt extract on non-associated haplotig aka q_contig == t_contig == False\n",
    "\n",
    "ph_p_homo_region -> True/False if primary contig coverage is 1x when mapping against primary contigs and haplotig contigs\n",
    "\n",
    "overlap_p_on_h_mapping -> True/False if gene falls within a region of genome with an alternative allele present (based on Assembletics mapping of associated haplotigs on their respective primary contigs)\n",
    "\n",
    "\n",
    "**The followign might change based on your genome and your SNP caller:**\n",
    "\n",
    "Pst_E104_v1_ph_ctg.freebayes_SNP ->  True/False if CDS contains a SNP when mapping against primary contigs and haplotig contigs\n",
    "\n",
    "Pst_E104_v1_ph_ctg.freebayes_SNP_# -> number of SNPs in CDS\n",
    "\n",
    "Pst_E104_v1_ph_ctg.freebayes_SNP_% -> of SNPs per bp  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input in this script\n",
    "\n",
    "For now script runs on Pst_104E_v12_p_ctg.no.Qcov80.PctID70.alleles and Pst_104E_v12_h_ctg.no.no_p_hits.Qcov80.PctID70.alleles. This ignores the fact that some h proteins are not the best hit of the p protein they are linked with.\n",
    "\n",
    "In addition. \n",
    "\n",
    "Coverage analysis needs a bed file for the homozygous coverage regions of primary contigs.\n",
    "\n",
    "Exonerate and a bunch of modules shown two lines down.\n",
    "\n",
    "Several PATH variable and file endings can be changed after the definition of all functions.\n",
    "\n",
    "##### Output\n",
    "\n",
    "It will be best to re-load the summary dataframe e.g. Pst_104E_v12_ph_ctg.no_alleles_QC.Qcov80.PctID70.df from genome.no_alleles.QcovXX.PctIDYY.df and do some analysis on this. Once we decided on how to proceed with this write out several no_allele scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/Bio/SearchIO/__init__.py:211: BiopythonExperimentalWarning: Bio.SearchIO is an experimental submodule which may undergo significant changes prior to its future official release.\n",
      "  BiopythonExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "import pysam\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import pybedtools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "from Bio.Seq import Seq\n",
    "import pysam\n",
    "from Bio import SearchIO\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second version of this allele analysis in which the initial alleles were based on proteinortho analysis and blast filtering.\n",
    "\n",
    "In order to stay consistent with the proteinortho output we will use the non-filtered output from the 'alleles_proteinortho' folder. In addition, the ph_ctg.proteinortho is read in to identify all genemodels with orthologs in general. This is used to generate a new column in the QC dataframe called 'singeltons' meaning having no ortholog in the other genome.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blast_outfmt6_to_bed(x):\n",
    "    \"Quick function that converts a blast outfmt6 file to a bed file.\"\n",
    "    blast_fo = open(x, 'r')\n",
    "    blast_lines = blast_fo.readlines()\n",
    "    bed_file_name = x + '.bed'\n",
    "    bed_fo = open(bed_file_name, 'w+')\n",
    "    for l in blast_lines:\n",
    "        content = l.split('\\t')\n",
    "        if int(content[8]) - int(content[9]) < 1:\n",
    "            print(content[1], int(content[8]) -1, content[9], content[0], content[10], \"+\", sep=\"\\t\", file=bed_fo) \n",
    "        else:\n",
    "            print(content[1], int(content[9]) -1, content[8],  content[0], content[10], \"-\", sep = \"\\t\", file=bed_fo)\n",
    "    blast_fo.close()\n",
    "    bed_fo.close()\n",
    "    return bed_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pwh_set = []\n",
    "def pwh_filter (q_contig, pwh_set=pwh_set):\n",
    "    '''Checks if contig belongs to the primary with haplotig set.'''\n",
    "    if q_contig in pwh_set:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same_contig_blast(x,y):\n",
    "    '''Function that checks if the blast hit in columne x is on the same contig as the the query sequence in\n",
    "    column y.\n",
    "    '''\n",
    "    q_contig = x.split('.')[2].split('_')[1]\n",
    "    hit_contig = y.split('_')[1]\n",
    "    if q_contig == hit_contig:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_primary_contig (q_contig):\n",
    "    '''Quick function that checks if query is on primary contig or not'''\n",
    "    if q_contig.startswith('hcontig'):\n",
    "        return False\n",
    "    elif q_contig.startswith('pcontig'):\n",
    "        return True\n",
    "    else:\n",
    "        print('Contig annotation needs to start with hcontig or pcontig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exn_asso_contig(query):\n",
    "    '''Quick function thats the summary of exonerate on on associated contig to the df'''\n",
    "    if query in exonerate_no_filtered_allele_asso_contig_bool_dict.keys():\n",
    "        return exonerate_no_filtered_allele_asso_contig_bool_dict[query]\n",
    "    else:\n",
    "        return 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exn_no_asso_contig(query):\n",
    "    '''Quick function thats the summary of exonerate on on associated contig to the df'''\n",
    "    if query in exonerate_no_filtered_allele_no_asso_contig_bool_dict.keys():\n",
    "        return exonerate_no_filtered_allele_no_asso_contig_bool_dict[query]\n",
    "    else:\n",
    "        return 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col_8_id(x):\n",
    "    '''Function that pulls out the ID from the 9th column of a df.'''\n",
    "    pattern = r'ID=([a-zA-Z0-9_.]*);'\n",
    "    regex = re.compile(pattern)  \n",
    "    m = regex.search(x)\n",
    "    match = m.groups()[0].replace('TU', 'model')\n",
    "    if match.startswith('cds.'):\n",
    "        match = match[4:]\n",
    "    if 'exon' in match:\n",
    "        _list = match.split('.')\n",
    "        match = '.'.join(_list[:-1])\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_SNP_dict ={}\n",
    "def number_of_SNPs(protein_id, tmp_SNP_dict=tmp_SNP_dict):\n",
    "    if protein_id in tmp_SNP_dict.keys():\n",
    "        return tmp_SNP_dict[protein_id]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENV parameters and Qcov and PctID cut_offs to define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define ENV parameters for blast hits and threads used in blast analysis\n",
    "n_threads = 4\n",
    "e_value = 1e-2\n",
    "blast_stderr_dict ={} #keep track of all the blast outputs and errors if so\n",
    "#here enter the Qcov and PctID cut off you would like to get analyzed. \n",
    "Qcov_cut_off = 80 #this defines the mimimum coverage of the Query to be required for filtering. Will become part of name.\n",
    "PctID_cut_off = 70 #this defines the mimimum PctID accross the alignment to be required for filtering. Will become part of name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATH variables to define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the PATH\n",
    "BASE_AA_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12'\n",
    "BASE_A_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/032017_assembly'\n",
    "BLAST_RESULT_PATH = os.path.join(BASE_AA_PATH,'allele_analysis' )\n",
    "ALLELE_PATH =os.path.join(BASE_AA_PATH ,'allele_analysis/alleles_proteinortho_graph516')\n",
    "BLAST_DB = os.path.join(BASE_AA_PATH, 'blast_DB')\n",
    "OUT_PATH = os.path.join(BASE_AA_PATH, 'allele_analysis', 'no_alleles_proteinortho_graph516_QC_Qcov%s_PctID%s_evalue01'% (Qcov_cut_off, PctID_cut_off))\n",
    "OUT_PATH_ELSE = os.path.join(OUT_PATH, 'maybe_useful')\n",
    "OUT_PATH_tmp = os.path.join(OUT_PATH, 'tmp')\n",
    "EXONERATE_PATH = os.path.join(OUT_PATH_tmp, 'exonerate')\n",
    "COV_PATH = os.path.join(BASE_AA_PATH, 'COV')\n",
    "VCF_SRM_PATH = os.path.join(BASE_AA_PATH, 'SRM_VCF')\n",
    "PROTEINORTHO = os.path.join(BASE_AA_PATH, 'proteinortho')\n",
    "proteinortho_fn = os.path.join(PROTEINORTHO, 'ph_ctg_516.proteinortho')\n",
    "proteinortho_graph_fn = os.path.join(PROTEINORTHO,'ph_ctg_516.proteinortho-graph')\n",
    "if not os.path.isdir(OUT_PATH):\n",
    "    os.mkdir(OUT_PATH)\n",
    "if not os.path.isdir(OUT_PATH_tmp):\n",
    "    os.mkdir(OUT_PATH_tmp)\n",
    "if not os.path.isdir(EXONERATE_PATH):\n",
    "    os.mkdir(EXONERATE_PATH)\n",
    "if not os.path.exists(OUT_PATH_ELSE):\n",
    "    os.mkdir(OUT_PATH_ELSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script variables to define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean up the tmp folder?\n",
    "clean_up = True #True will delete the tmp folder with tmp blast hits and exonerate output files\n",
    "exonerate_script_name = 'exonerate_alignments_vulgar.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genome IDs to enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#genome\n",
    "p_genome = 'Pst_104E_v12_p_ctg'\n",
    "h_genome = 'Pst_104E_v12_h_ctg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For COV and SNP analysis enter bed file names or endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homo_cov_ph_p = '.ph_p_homo_cov.bed' #this is the coverage bed file which defines regions that have homozogous coverage\n",
    "                                    #when doing ph mapping on p\n",
    "vcf_file_endings = '.DP10Q20.vcf' #this hould be your filter settings for SNP calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protein_fa_files = [os.path.join(BASE_A_PATH, x) for x in os.listdir(BASE_A_PATH) if x.endswith('protein.fa')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in protein ids for p and h contigs and store names in a list in a dict with unique key id [first part of\n",
    "#file name].\n",
    "fa_protein_dict = {}\n",
    "fa_protein_seq_dict = {}\n",
    "fa_protein_length_dict = {}\n",
    "for file in protein_fa_files:\n",
    "    seq_list = []\n",
    "    length_list =[]\n",
    "    for seq in SeqIO.parse(open(file), 'fasta'):\n",
    "        fa_protein_seq_dict[seq.id] = seq\n",
    "        fa_protein_length_dict[seq.id] = len(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the file names of the no allele cases including the filtered settings with Qcov and PctID cut offs and the no alleles\n",
    "#at all that in principle is all p and h proteins without a blast hit with a given e-value right now 0.001\n",
    "filtered_no_alleles = []\n",
    "#updated catching the no allele files\n",
    "for x in os.listdir(ALLELE_PATH):\n",
    "    if 'p_hits' not in x:\n",
    "        if 'Qcov' in x:\n",
    "            if x.endswith('no_alleles'):\n",
    "                filtered_no_alleles.append(os.path.join(ALLELE_PATH, x) )\n",
    "\n",
    "\n",
    "        \n",
    "filtered_no_alleles_dict = {}\n",
    "for x in filtered_no_alleles:\n",
    "    key = x.split('/')[-1].split('.')[0]\n",
    "    filtered_no_alleles_dict[key] = x\n",
    "\n",
    "#updated catching the no allele files\n",
    "no_alleles_at_all = []\n",
    "\n",
    "for x in os.listdir(ALLELE_PATH):\n",
    "    if 'p_hits' not in x:\n",
    "        if 'Qcov' not in x:\n",
    "            if x.endswith('no_alleles'):\n",
    "                no_alleles_at_all.append(os.path.join(ALLELE_PATH, x) )\n",
    "\n",
    "no_alleles_at_all_dict ={}\n",
    "for x in no_alleles_at_all:\n",
    "    key = x.split('/')[-1].split('.')[0]\n",
    "    no_alleles_at_all_dict[key] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Might want to be incooporated in the script in future\n",
    "Pull gff and genome fasta files over into the tmp folder make gene gff and pull out gene sequences with bedtools getfasta on the command line using subproccesses. See below ideas from original script\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#now pull gene sequences for no-besties and do blast on the corresponding other haplotype\n",
    "#gene files were generated by \n",
    "cat Pst_E104_v1_p_ctg.anno.RepaseTPSI_filtered.gff3 | awk '$3==\"gene\"' > Pst_E104_v1_p_ctg.gene.RepaseTPSI_filtered.gff3\n",
    "for gff in gene_gff:\n",
    "    gene = ''\n",
    "    gene_df = pd.read_csv(folder + gff, header = None, sep='\\t' )\n",
    "    gene_df[2] = gene_df[8].apply(col_8_id)\n",
    "    gene_df.to_csv(folder+gff, header=None, sep='\\t', index=None)\n",
    "bedtools getfasta -s -name -fi Pst_E104_v1_ph_ctg.fa -bed Pst_E104_v1_ph_ctg.gene.RepaseTPSI_filtered.gff3 -fo Pst_E104_v1_ph_ctg.gene.RepaseTPSI_filtered.fa\n",
    "bedtools getfasta -s -name -bed Pst_E104_v1_h_ctg.gene.RepaseTPSI_filtered.gff3 -fi Pst_E104_v1_h_ctg.fa -fo Pst_E104_v1_h_ctg.gene.RepaseTPSI_filtered.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the gene.fa files and put them in a dict that has the genome as a key\n",
    "gene_fa_files = [os.path.join(BASE_A_PATH, x) for x in os.listdir(BASE_A_PATH) if x.endswith('gene.fa')]\n",
    "gene_fa_files_dict = {}\n",
    "for x in gene_fa_files:\n",
    "    key = x.split('/')[-1].split('.')[0]\n",
    "    gene_fa_files_dict[key] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All databases generated and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#generate the blast databases if not already present\n",
    "os.chdir(BLAST_DB)\n",
    "blast_dir_content = os.listdir(BLAST_DB)\n",
    "for x in blast_dir_content:\n",
    "    if x.endswith('.fa') and ({os.path.isfile(x + e) for e in ['.psq', '.phr', '.pin'] } != {True}\\\n",
    "           and {os.path.isfile(x + e) for e in ['.nin', '.nhr', '.nsq'] } != {True} ):\n",
    "\n",
    "        make_DB_options = ['-in']\n",
    "        make_DB_options.append(x)\n",
    "        make_DB_options.append('-dbtype')\n",
    "        if 'protein' in x:\n",
    "            make_DB_options.append('prot')\n",
    "        else:\n",
    "            make_DB_options.append('nucl')\n",
    "        make_DB_command = 'makeblastdb %s' % ' '.join(make_DB_options)\n",
    "        make_DB_stderr = subprocess.check_output(make_DB_command, shell=True, stderr=subprocess.STDOUT)\n",
    "        print('%s is done!' % make_DB_command)\n",
    "print(\"All databases generated and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the blast db files and put them in a dict that has the genome as a key\n",
    "gene_blast_db = [os.path.join(BLAST_DB, x) for x in os.listdir(BLAST_DB) if x.endswith('gene.fa')]\n",
    "gene_blast_db_dict ={}\n",
    "for x in gene_blast_db:\n",
    "    key = x.split('/')[-1].split('.')[0]\n",
    "    gene_blast_db_dict[key] = x\n",
    "genome_blast_db = [os.path.join(BLAST_DB, x) for x in os.listdir(BLAST_DB) if x.endswith('_ctg.fa')]\n",
    "genome_blast_db_dict ={}\n",
    "for x in genome_blast_db:\n",
    "    key = x.split('/')[-1].split('.')[0]\n",
    "    genome_blast_db_dict[key] = x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the proteinortho file we will start with the unfiltered restults from the allele analysis. This will aid with being consistent. Hence in the next line filtered_no_alleles_dict will be set to no_alleles_at_all dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pst_104E_v12_h_ctg': '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/allele_analysis/alleles_proteinortho_graph516/Pst_104E_v12_h_ctg.Qcov80.PctID70.no_alleles',\n",
       " 'Pst_104E_v12_p_ctg': '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/allele_analysis/alleles_proteinortho_graph516/Pst_104E_v12_p_ctg.Qcov80.PctID70.no_alleles'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_no_alleles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pst_104E_v12_h_ctg': '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/allele_analysis/alleles_proteinortho_graph516/Pst_104E_v12_h_ctg.no_alleles',\n",
       " 'Pst_104E_v12_p_ctg': '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/allele_analysis/alleles_proteinortho_graph516/Pst_104E_v12_p_ctg.no_alleles'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_alleles_at_all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filtered_no_alleles_dict = no_alleles_at_all_dict\n",
    "#actually don't do this for now and simply start with the broader approach we can filter down\n",
    "#the dataframe afterwards more easily once it is all done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One pair of filtered non-allele files given. Good to go!\n"
     ]
    }
   ],
   "source": [
    "#using the dictionary approach to stich together all the different input files. The key is always the genome. In this cases\n",
    "#being the part of the file name before the first '.'\n",
    "if len(filtered_no_alleles) != 2:\n",
    "    print(\"This script right now is only designed for one set of filter files.\")\n",
    "    print(\"Please hold!\")\n",
    "else:\n",
    "    print(\"One pair of filtered non-allele files given. Good to go!\")\n",
    "    \n",
    "#simply pulls in the gene sequences of missing alleles. Do this on the filtered set as the unfiltered set is a subset anyway\n",
    "no_filtered_allele_gene_dict = {}\n",
    "for no_alleles_key in filtered_no_alleles_dict.keys():\n",
    "    #read in all the alleles from file this assumes that only one filter setting was run in the allele folder\n",
    "    no_filtered_allele_list = pd.read_csv(os.path.join(ALLELE_PATH, filtered_no_alleles_dict[no_alleles_key]), header=None, sep='\\t')[0].tolist()\n",
    "    #convert from proteins ids to gene ideas\n",
    "    no_filtered_allele_list =  [x.replace('evm.model', 'evm.TU') for x in no_filtered_allele_list]\n",
    "    \n",
    "    no_filtered_allele_seq = []\n",
    "    for seq in SeqIO.parse(open(gene_blast_db_dict[no_alleles_key]), 'fasta'):\n",
    "        if seq.id in no_filtered_allele_list:\n",
    "            no_filtered_allele_seq.append(seq)\n",
    "    #get the proper file name\n",
    "    out_f_prefix = filtered_no_alleles_dict[no_alleles_key].split('/')[-1]\n",
    "    out_f = out_f_prefix + '.gene.fa'\n",
    "    f_handle = open(os.path.join(OUT_PATH_ELSE, out_f),'w') #need to generate handle for writing and\n",
    "    SeqIO.write(no_filtered_allele_seq, f_handle, 'fasta')\n",
    "    f_handle.close() #closing file afterwards again\n",
    "    no_filtered_allele_gene_dict[no_alleles_key] = os.path.join(OUT_PATH_ELSE, out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blastn -query /home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/allele_analysis/no_alleles_proteinortho_graph516_QC_Qcov80_PctID70_evalue01/maybe_useful/Pst_104E_v12_p_ctg.Qcov80.PctID70.no_alleles.gene.fa -db /home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/blast_DB/Pst_104E_v12_h_ctg.fa -outfmt 6 -evalue 0.01 -num_threads 4 > /home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/allele_analysis/no_alleles_proteinortho_graph516_QC_Qcov80_PctID70_evalue01/tmp/Pst_104E_v12_p_ctg.Qcov80.PctID70.no_alleles.gene.fa.db_Pst_104E_v12_h_ctg.fa.0.01.blastn.outfmt6\n",
      "New blast run and done!\n",
      "blastn -query /home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/allele_analysis/no_alleles_proteinortho_graph516_QC_Qcov80_PctID70_evalue01/maybe_useful/Pst_104E_v12_h_ctg.Qcov80.PctID70.no_alleles.gene.fa -db /home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/blast_DB/Pst_104E_v12_p_ctg.fa -outfmt 6 -evalue 0.01 -num_threads 4 > /home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/allele_analysis/no_alleles_proteinortho_graph516_QC_Qcov80_PctID70_evalue01/tmp/Pst_104E_v12_h_ctg.Qcov80.PctID70.no_alleles.gene.fa.db_Pst_104E_v12_p_ctg.fa.0.01.blastn.outfmt6\n",
      "New blast run and done!\n"
     ]
    }
   ],
   "source": [
    "#do the gene against other haplotype blast\n",
    "no_filtered_allele_gene_genome_blast_dict ={}\n",
    "for no_alleles_key in no_filtered_allele_gene_dict.keys():\n",
    "    blast_options = ['-query']\n",
    "    query = no_filtered_allele_gene_dict[no_alleles_key]\n",
    "    blast_options.append(query)\n",
    "    blast_options.append('-db')\n",
    "    if no_alleles_key == p_genome:\n",
    "        db = genome_blast_db_dict[h_genome]\n",
    "    elif no_alleles_key == h_genome:\n",
    "        db = genome_blast_db_dict[p_genome]\n",
    "    else:\n",
    "        print(\"There is something wrong with the file name prefixes and the genome (h and p) provided!\")\n",
    "    blast_options.append(db)\n",
    "    blast_options.append('-outfmt 6')\n",
    "    blast_options.append('-evalue')\n",
    "    blast_options.append(str(e_value))\n",
    "    blast_options.append('-num_threads')\n",
    "    blast_options.append(str(n_threads))\n",
    "    #blast_options.append('-max_target_seqs 1')\n",
    "    blast_options.append('>')\n",
    "    if 'gene' in query:\n",
    "        out_name_list = [ query.split('/')[-1], 'db_' + db.split('/')[-1], str(e_value), 'blastn.outfmt6']\n",
    "        out_name = os.path.join(OUT_PATH_tmp ,'.'.join(out_name_list))\n",
    "        blast_options.append(out_name)\n",
    "        blast_command = 'blastn %s' % ' '.join(blast_options)\n",
    "    no_filtered_allele_gene_genome_blast_dict[no_alleles_key] = out_name\n",
    "    print(blast_command)\n",
    "    if not os.path.exists(out_name):\n",
    "        blast_stderr_dict[blast_command] = subprocess.check_output(blast_command, shell=True, stderr=subprocess.STDOUT)\n",
    "        print(\"New blast run and done!\")\n",
    "    else:\n",
    "        blast_stderr_dict[blast_command] = 'Previously done already!'\n",
    "        print('Previously done already!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now convert all the gene level against genome blast hits to bed files\n",
    "no_filtered_allele_gene_genome_blast_bed_dict = {}\n",
    "for key, value in no_filtered_allele_gene_genome_blast_dict.items():\n",
    "    no_filtered_allele_gene_genome_blast_bed_dict[key] = blast_outfmt6_to_bed(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here track what happens with the no_alleles. Meaning how many of those have a gene vs. genome hit and how many don't \n",
    "\n",
    "#these dict will hold the list of SeqIO.records of blast hit regions for each no_allele hiting the other haplotig split into\n",
    "#the id of the contig will be h/pcontig_xxx_start_end of DNA sequence\n",
    "\n",
    "#hit on associated contig\n",
    "no_filtered_allele_gene_genome_hit_asso_contig_dict = {}\n",
    "\n",
    "#hit on unlinked contigs\n",
    "no_filtered_allele_gene_genome_hit_no_asso_contig_dict = {}\n",
    "\n",
    "no_filtered_allele_gene_no_genome_hit_dict ={}\n",
    "\n",
    "for key, no_filtered_alllele_fn in filtered_no_alleles_dict.items():\n",
    "\n",
    "    no_filtered_alleles = pd.read_csv(no_filtered_alllele_fn, sep='\\t', header=None)[0].unique()\n",
    "    genome_hits_header = ['Contig', 'start', 'end', 'blast_query', 'e-value', 'strand']\n",
    "    gene_genome_hits_df = pd.read_csv(no_filtered_allele_gene_genome_blast_bed_dict[key], sep='\\t', \\\n",
    "                                         names = genome_hits_header, header=None)\n",
    "    gene_genome_hits_df['Protein_ID'] = gene_genome_hits_df['blast_query'].str.replace('evm.TU', 'evm.model')\n",
    "    #get all alleles with no gene vs. genome hit and save them to file with ending 'no_allele_no_gene_genome_blast_hit.txt'\n",
    "    no_filtered_allele_gene_no_genome_hit = np.setdiff1d(no_filtered_alleles, gene_genome_hits_df.Protein_ID.unique(), assume_unique= True)\n",
    "    out_fn = os.path.join(OUT_PATH, key +'.no_allele_no_gene_genome_blast_hit.txt')\n",
    "    no_filtered_allele_gene_no_genome_hit_dict[key] = out_fn\n",
    "    np.savetxt(out_fn, no_filtered_allele_gene_no_genome_hit, fmt='%s')\n",
    "    #now filter out the best hit on an associated contig\n",
    "    gene_genome_hits_df['asso_contig'] = gene_genome_hits_df['blast_query'].combine(gene_genome_hits_df['Contig'], func=same_contig_blast)\n",
    "    tmp_same_contig_df = ''\n",
    "    tmp_same_contig_df = gene_genome_hits_df[gene_genome_hits_df['asso_contig'] == True]\n",
    "    #now filter out the best hit on an not-associated contig <- not for now as this might get a bit complicated with paraglogs and such\n",
    "    tmp_diff_contig_df = ''\n",
    "    tmp_diff_contig_df_grouped = gene_genome_hits_df[gene_genome_hits_df['asso_contig'] == False].groupby('blast_query')\n",
    "    tmp_diff_contig_best_hits = tmp_diff_contig_df_grouped.apply(lambda g: g[g['e-value'] == g['e-value'].min()])\n",
    "    #now get all query protein ids\n",
    "    tmp_protein_id = gene_genome_hits_df['Protein_ID'].unique()\n",
    "    genome_name = ''\n",
    "    if key == p_genome:\n",
    "        genome_name = os.path.join(BASE_A_PATH, h_genome+'.fa')\n",
    "    elif key == h_genome:\n",
    "        genome_name = os.path.join(BASE_A_PATH, p_genome+'.fa')\n",
    "    genome_fa = pysam.FastaFile(genome_name)\n",
    "    \n",
    "    for protein_id in tmp_protein_id:\n",
    "        \n",
    "        #now loop through the protein_ids of no_alleles hiting the associated contig aka same contig\n",
    "        #could do something like gene_genome_hits_df.pivot_table(columns=['Protein_ID', 'Contig'], aggfunc={'start' : 'min', 'end':'min'})\n",
    "        tmp_protein_id_df = gene_genome_hits_df[(gene_genome_hits_df['Protein_ID'] == protein_id) & (gene_genome_hits_df['asso_contig'] == True) ]\n",
    "        \n",
    "        if len(tmp_protein_id_df) < 1:\n",
    "            continue\n",
    "        tmp_hit_contig = tmp_protein_id_df[\"Contig\"].unique()\n",
    "        tmp_gene_genome_seq_list = [] #saves SeqIO records from blast hits and suroundings\n",
    "        #now loop through the associated contig hits incase we have multiple associated contigs hit\n",
    "        for hit in tmp_hit_contig:\n",
    "            tmp_df_2 = tmp_protein_id_df[tmp_protein_id_df['Contig'] == hit]\n",
    "            #get the smallest starting point on the specific contig\n",
    "            start = tmp_df_2['start'].min() - 30000\n",
    "            if start < 1:\n",
    "                start = 1\n",
    "            end = tmp_df_2['end'].max() + 30000\n",
    "            seq = genome_fa.fetch(hit, start, end)\n",
    "            seq_r = '' #initialize empty SeqIO record\n",
    "            seq_id = hit + '_' + str(start) + '_' + str(end)\n",
    "            seq_ob = Seq(seq)\n",
    "            seq_ob.alphabet = 'fasta'\n",
    "            seq_r = SeqRecord(seq_ob)\n",
    "            seq_r.id = seq_id\n",
    "            tmp_gene_genome_seq_list.append(seq_r)\n",
    "        no_filtered_allele_gene_genome_hit_asso_contig_dict[protein_id] = tmp_gene_genome_seq_list\n",
    "        \n",
    "        \n",
    "    #need to loop through the protein_ids twice as the len(tmp_protein_id_df <1) introduces a silent error for \n",
    "    #hits with only not associated contigs\n",
    "    for protein_id in tmp_protein_id:   \n",
    "        #now loop through the protein_ids of no_alleles hiting unassociated contig aka diff_contig\n",
    "        tmp_protein_id_df = tmp_diff_contig_best_hits[(tmp_diff_contig_best_hits['Protein_ID'] == protein_id)]\n",
    "        if len(tmp_protein_id_df) < 1:\n",
    "            continue\n",
    "        tmp_hit_contig = tmp_protein_id_df[\"Contig\"].unique()\n",
    "        tmp_gene_genome_seq_list = [] #saves SeqIO records from blast hits and suroundings\n",
    "        #now loop through the associated contig hits incase we have multiple associated contigs hit\n",
    "        #pull out the blast hit regions (for one contig start(min) and end(max) if mulitple hits on same contig.\n",
    "        #save SeqIO.Records for each protein_id in a list\n",
    "        for hit in tmp_hit_contig:\n",
    "            tmp_df_2 = tmp_protein_id_df[tmp_protein_id_df['Contig'] == hit]\n",
    "            #get the smallest starting point on the specific contig\n",
    "            start = tmp_df_2['start'].min() - 30000\n",
    "            if start < 1:\n",
    "                start = 1\n",
    "            end = tmp_df_2['end'].max() + 30000\n",
    "            seq = genome_fa.fetch(hit, start, end)\n",
    "            seq_r = '' #initialize empty SeqIO record\n",
    "            seq_id = hit + '_' + str(start) + '_' + str(end)\n",
    "            seq_ob = Seq(seq)\n",
    "            seq_ob.alphabet = 'fasta'\n",
    "            seq_r = SeqRecord(seq_ob)\n",
    "            seq_r.id = seq_id\n",
    "            tmp_gene_genome_seq_list.append(seq_r)\n",
    "        no_filtered_allele_gene_genome_hit_no_asso_contig_dict[protein_id] = tmp_gene_genome_seq_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now write an exonerate script that aligns the protein sequences to the DNA sequences\n",
    "EXONERATE_PATH_asso = os.path.join(EXONERATE_PATH, 'hit_associated_contigs')\n",
    "EXONERATE_PATH_no_asso = os.path.join(EXONERATE_PATH, 'hit_nonassociated_contigs')\n",
    "if not os.path.exists(EXONERATE_PATH_asso):\n",
    "    os.mkdir(EXONERATE_PATH_asso)\n",
    "if not os.path.exists(EXONERATE_PATH_no_asso):\n",
    "    os.mkdir(EXONERATE_PATH_no_asso)\n",
    "#open up the script\n",
    "exonerate_script = os.path.join(OUT_PATH, exonerate_script_name)\n",
    "out_exonerate = open(exonerate_script, 'w')\n",
    "out_exonerate.write('#!/bin/bash\\n')\n",
    "for contig_key, contig_seq_list in no_filtered_allele_gene_genome_hit_asso_contig_dict.items():\n",
    "    out_folder = os.path.join(EXONERATE_PATH_asso, contig_key)\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "    out_protein_fn = os.path.join(out_folder, contig_key + '.fa')\n",
    "    out_handle = open(out_protein_fn, 'w')\n",
    "    #write down the protein sequence\n",
    "    SeqIO.write(fa_protein_seq_dict[contig_key], out_handle, 'fasta')\n",
    "    out_handle.close()\n",
    "    #write the exonerate script\n",
    "    out_exonerate.write('cd %s\\n'% out_folder)\n",
    "    #write out all the genomic regions\n",
    "    for seq in contig_seq_list:\n",
    "        out_seq_name = os.path.join(out_folder, seq.id +'.fa')\n",
    "        out_seq_handle = open(out_seq_name, 'w')\n",
    "        SeqIO.write(seq, out_seq_handle, 'fasta')\n",
    "        out_seq_handle.close()\n",
    "        #write exonerate script the command\n",
    "        out_exonerate.write('exonerate --model protein2genome --percent 20 -q %s -t %s --showalignment False -S > %s.vulgar_exn\\n'\\\n",
    "                           %(out_protein_fn, out_seq_name, out_seq_name))\n",
    "\n",
    "    #out_exonerate.write('cd %s\\n'% out_folder) #not necessary\n",
    "    \n",
    "for contig_key, contig_seq_list in no_filtered_allele_gene_genome_hit_no_asso_contig_dict.items():\n",
    "    out_folder = os.path.join(EXONERATE_PATH_no_asso, contig_key)\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "    out_protein_fn = os.path.join(out_folder, contig_key + '.fa')\n",
    "    out_handle = open(out_protein_fn, 'w')\n",
    "    #write down the protein sequence\n",
    "    SeqIO.write(fa_protein_seq_dict[contig_key], out_handle, 'fasta')\n",
    "    out_handle.close()\n",
    "    #write the exonerate script\n",
    "    out_exonerate.write('cd %s\\n'% out_folder)\n",
    "    #write out all the genomic regions\n",
    "    for seq in contig_seq_list:\n",
    "        out_seq_name = os.path.join(out_folder, seq.id +'.fa')\n",
    "        out_seq_handle = open(out_seq_name, 'w')\n",
    "        SeqIO.write(seq, out_seq_handle, 'fasta')\n",
    "        out_seq_handle.close()\n",
    "        #write exonerate script the command\n",
    "        out_exonerate.write('exonerate --model protein2genome --percent 20 -q %s -t %s --showalignment False -S > %s.vulgar_exn\\n'\\\n",
    "                           %(out_protein_fn, out_seq_name, out_seq_name))\n",
    "\n",
    "    #out_exonerate.write('cd %s\\n'% out_folder)       \n",
    "\n",
    "out_exonerate.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exonerate script run successfully\n"
     ]
    }
   ],
   "source": [
    "#now run the exonerate script\n",
    "exonerate_command = 'bash %s' % exonerate_script\n",
    "exonerate_stderr = subprocess.check_output(exonerate_command , shell=True, stderr=subprocess.STDOUT)\n",
    "print('Exonerate script run successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#no loop through the exonerate vulgar result and generate a dictionray of the results\n",
    "#if hsps query range == (0, query_length) and not F in .vulgar_comp it is likely that the alignment is actually good\n",
    "#and and the gene model might have been dropped for another reason\n",
    "#a dict that has the protein ID as key and the results of exonerate as list as value for each contig [contig : True/False].\n",
    "exonerate_best_hit_dict = {}\n",
    "exonerate_no_filtered_allele_gene_genome_hit_asso_contig_dict = {}\n",
    "exonerate_no_filtered_allele_asso_contig_bool_dict = {}\n",
    "\n",
    "\n",
    "#generate a best hit dict dummy place holder for each contig key\n",
    "for contig_key in set(list(no_filtered_allele_gene_genome_hit_asso_contig_dict.keys())\\\n",
    "                      + list(no_filtered_allele_gene_genome_hit_no_asso_contig_dict.keys())):\n",
    "    exonerate_best_hit_dict[contig_key] = ['dummy : 0']\n",
    "\n",
    "\n",
    "\n",
    "#now loop through the exonerate folders\n",
    "for contig_key in no_filtered_allele_gene_genome_hit_asso_contig_dict.keys():\n",
    "    out_folder = os.path.join(EXONERATE_PATH_asso, contig_key)\n",
    "    query_length = fa_protein_length_dict[contig_key]\n",
    "    #the results list will store the result for each individual exonerate alignment as boolean value. \n",
    "    #True == alignment successful (alignment range == range length protein sequence, no F(rameshit) in vulgar string)\n",
    "    exonerate_result_list = []\n",
    "    counter = 0\n",
    "    overall_best_score = 0\n",
    "    overall_best_hit = ''\n",
    "    #get all vulgar alignment results\n",
    "    vulgar_exn_list = [os.path.join(out_folder, x) for x in os.listdir(out_folder) if x.endswith('vulgar_exn')]\n",
    "    opt_query_range = (0, query_length)\n",
    "    #loop through vulgar parser and see if hit is valid \n",
    "    for fname in vulgar_exn_list:\n",
    "        best_score = 0\n",
    "        best_hit = ''\n",
    "        result = SearchIO.parse(fname, 'exonerate-vulgar')\n",
    "        genome_region = fname.split('/')[-1].split('.')[0]\n",
    "        for hit in result:\n",
    "            #loop through all hsps hits\n",
    "            for hsps in hit.hsps:\n",
    "                hsps_range = hsps.query_range\n",
    "                vulgar_list = hsps.vulgar_comp.strip(' ').split(' ')\n",
    "                #print(hsps_range, vulgar_list)\n",
    "                #this is the contition for something being a potential protein alignment that\n",
    "                #True == alignment successful (alignment range == range length protein sequence, \\\n",
    "                #no F(rameshit) in vulgar string)\n",
    "                if hsps_range == opt_query_range and 'F' not in vulgar_list:\n",
    "                    counter += 1\n",
    "                    if hsps.score > best_score:\n",
    "                        best_hit = hsps.hit_id\n",
    "                        best_score = hsps.score\n",
    "                    if hsps.score > overall_best_score:\n",
    "                        overall_best_hit = hsps.hit_id\n",
    "                        overall_best_score = hsps.score\n",
    "                    #print(key)\n",
    "        if best_score > 0:\n",
    "            exonerate_result_list.append('%s : True' % genome_region)\n",
    "        else:\n",
    "            exonerate_result_list.append('%s : False' % genome_region)\n",
    "            \n",
    "    exonerate_no_filtered_allele_gene_genome_hit_asso_contig_dict[contig_key] = exonerate_result_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    if counter > 0:\n",
    "        exonerate_no_filtered_allele_asso_contig_bool_dict[contig_key] = True\n",
    "        \n",
    "        if contig_key in exonerate_best_hit_dict.keys():\n",
    "            if int(exonerate_best_hit_dict[contig_key][0].split(':')[1][1:]) < overall_best_score:\n",
    "                exonerate_best_hit_dict[contig_key] = ['%s : %s' % (overall_best_hit, overall_best_score)]\n",
    "    else:\n",
    "        exonerate_no_filtered_allele_asso_contig_bool_dict[contig_key] = False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#no loop through the exonerate vulgar result and generate a dictionray of the results\n",
    "#if hsps query range == (0, query_length) and not F in .vulgar_comp it is likely that the alignment is actually good\n",
    "#and and the gene model might have been dropped for another reason\n",
    "#a dict that has the protein ID as key and the results of exonerate as list as value for each contig [contig : True/False].\n",
    "exonerate_no_filtered_allele_gene_genome_hit_no_asso_contig_dict = {}\n",
    "exonerate_no_filtered_allele_no_asso_contig_bool_dict = {}\n",
    "#now loop through the exonerate folders\n",
    "\n",
    "\n",
    "for contig_key in no_filtered_allele_gene_genome_hit_no_asso_contig_dict.keys():\n",
    "    out_folder = os.path.join(EXONERATE_PATH_no_asso, contig_key)\n",
    "    query_length = fa_protein_length_dict[contig_key]\n",
    "    #the results list will store the result for each individual exonerate alignment as boolean value. \n",
    "    #True == alignment successful (alignment range == range length protein sequence, no F(rameshit) in vulgar string)\n",
    "    exonerate_result_list = []\n",
    "    counter = 0\n",
    "    overall_best_score = 0\n",
    "    overall_best_hit = ''\n",
    "    #get all vulgar alignment results\n",
    "    vulgar_exn_list = [os.path.join(out_folder, x) for x in os.listdir(out_folder) if x.endswith('vulgar_exn')]\n",
    "    opt_query_range = (0, query_length)\n",
    "    #loop through vulgar parser and see if hit is valid \n",
    "    for fname in vulgar_exn_list:\n",
    "        best_score = 0\n",
    "        best_hit = ''\n",
    "        result = SearchIO.parse(fname, 'exonerate-vulgar')\n",
    "        genome_region = fname.split('/')[-1].split('.')[0]\n",
    "        for hit in result:\n",
    "            #loop through all hsps hits\n",
    "            for hsps in hit.hsps:\n",
    "                hsps_range = hsps.query_range\n",
    "                vulgar_list = hsps.vulgar_comp.strip(' ').split(' ')\n",
    "                #print(hsps_range, vulgar_list)\n",
    "                #this is the contition for something being a potential protein alignment that\n",
    "                #True == alignment successful (alignment range == range length protein sequence, \\\n",
    "                #no F(rameshit) in vulgar string)\n",
    "                if hsps_range == opt_query_range and 'F' not in vulgar_list:\n",
    "                    counter += 1\n",
    "                    if hsps.score > best_score:\n",
    "                        best_hit = hsps.hit_id\n",
    "                        best_score = hsps.score\n",
    "                    if hsps.score > overall_best_score:\n",
    "                        overall_best_hit = hsps.hit_id\n",
    "                        overall_best_score = hsps.score\n",
    "                    #print(key)\n",
    "        if best_score > 0:\n",
    "            exonerate_result_list.append('%s : True' % genome_region)\n",
    "        else:\n",
    "            exonerate_result_list.append('%s : False' % genome_region)\n",
    "            \n",
    "    exonerate_no_filtered_allele_gene_genome_hit_no_asso_contig_dict[contig_key] = exonerate_result_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    if counter > 0:\n",
    "        exonerate_no_filtered_allele_no_asso_contig_bool_dict[contig_key] = True\n",
    "        \n",
    "        if contig_key in exonerate_best_hit_dict.keys():\n",
    "            if int(exonerate_best_hit_dict[contig_key][0].split(':')[1][1:]) < overall_best_score:\n",
    "                exonerate_best_hit_dict[contig_key] = ['%s : %s' % (overall_best_hit, overall_best_score)]\n",
    "    else:\n",
    "        exonerate_no_filtered_allele_no_asso_contig_bool_dict[contig_key] = False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the exonerate dictonaries, combine vulgar results and delete all the exonerate files if clean up == True\n",
    "out_name = os.path.join(OUT_PATH_ELSE, p_genome.replace('p_ctg', 'ph_ctg') + 'exonerate_no_filtered_allele_asso_contig_bool_dict.txt')\n",
    "json.dump(exonerate_no_filtered_allele_asso_contig_bool_dict,open(out_name, 'w'))\n",
    "\n",
    "out_name = os.path.join(OUT_PATH_ELSE, p_genome.replace('p_ctg', 'ph_ctg') + 'exonerate_no_filtered_allele_no_asso_contig_bool_dict.txt')\n",
    "json.dump(exonerate_no_filtered_allele_no_asso_contig_bool_dict,open(out_name, 'w'))\n",
    "\n",
    "out_name = os.path.join(OUT_PATH_ELSE, p_genome.replace('p_ctg', 'ph_ctg') + 'exonerate_no_filtered_allele_gene_genome_hit_asso_contig_dict.txt')\n",
    "json.dump(exonerate_no_filtered_allele_gene_genome_hit_asso_contig_dict,open(out_name, 'w'))\n",
    "\n",
    "out_name = os.path.join(OUT_PATH_ELSE, p_genome.replace('p_ctg', 'ph_ctg') + 'exonerate_no_filtered_allele_gene_genome_hit_no_asso_contig_dict.txt')\n",
    "json.dump(exonerate_no_filtered_allele_gene_genome_hit_no_asso_contig_dict,open(out_name, 'w'))\n",
    "\n",
    "out_name = os.path.join(OUT_PATH_ELSE, p_genome.replace('p_ctg', 'ph_ctg') + 'exonerate_best_hit_dict.txt')\n",
    "json.dump(exonerate_best_hit_dict,open(out_name, 'w'))\n",
    "\n",
    "\n",
    "vulgar_files = glob.glob(os.path.join(EXONERATE_PATH, '*/*/*.vulgar_exn' ))\n",
    "out_name = os.path.join(OUT_PATH_ELSE, p_genome.replace('p_ctg','ph_ctg') +'exonerate_vulgar_exn_all.txt')\n",
    "with open(out_name, 'wb') as outfile:\n",
    "    for f in vulgar_files:\n",
    "        with open(f, 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "if clean_up == True:\n",
    "    shutil.rmtree(EXONERATE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:30: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:31: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:34: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "#now read in the initial blast dataframes, filter them down to all no_alleles_filtered\n",
    "blast_out_dict = {}\n",
    "blast_header = ['Query', 'Target', 'PctID', 'AlnLgth', 'NumMis', 'NumGap', 'StartQuery', 'StopQuery', 'StartTarget',\\\n",
    "              'StopTarget', 'e-value','BitScore']\n",
    "blastp_result_files = [os.path.join(BLAST_RESULT_PATH,x) for x in os.listdir(BLAST_RESULT_PATH) if x.endswith('outfmt6') and x.split('.')[-2] == 'blastp' ]\n",
    "blastp_results_dict = {}\n",
    "for x in blastp_result_files:\n",
    "    key = x.split('/')[-1].split('.')[0]\n",
    "    blastp_results_dict[key] = x\n",
    "\n",
    "for key, blastp_fn in blastp_results_dict.items():\n",
    "    tmp_df = pd.read_csv(blastp_fn, sep='\\t', header=None, names=blast_header)\n",
    "    tmp_no_allele_list = pd.read_csv(filtered_no_alleles_dict[key], sep ='\\t', header = None)[0].tolist()\n",
    "    tmp_df = tmp_df[tmp_df.Query.isin(tmp_no_allele_list)]\n",
    "    tmp_df[\"QLgth\"] = tmp_df[\"Query\"].apply(lambda x: fa_protein_length_dict[x])\n",
    "    tmp_df[\"QCov\"] = tmp_df['AlnLgth']/tmp_df['QLgth']*100\n",
    "    tmp_df.sort_values(by=['Query', 'e-value','BitScore', ],ascending=[True, True, False], inplace=True)\n",
    "    #now make sure to add proteins/genes without blast hit to the dataframes e.g. some of the no_alleles will have had no blast hit in the initial blast\n",
    "    tmp_all_queries_w_hit = tmp_df[\"Query\"].unique()\n",
    "    tmp_queries_no_hit = set(tmp_no_allele_list) - set(tmp_all_queries_w_hit)\n",
    "    no_hit_list = []\n",
    "    #loop over the quieres with no hit and make list of list out of them the first element being the query id\n",
    "    for x in tmp_queries_no_hit:\n",
    "        NA_list = ['False'] * len(tmp_df.columns)\n",
    "        NA_list[0] = x\n",
    "        no_hit_list.append(NA_list)\n",
    "    tmp_no_hit_df = pd.DataFrame(no_hit_list)\n",
    "    tmp_no_hit_df.columns = tmp_df.columns\n",
    "    tmp_no_hit_df['QLgth'] = tmp_no_hit_df.Query.apply(lambda x: fa_protein_length_dict[x])\n",
    "    tmp_df = tmp_df.append(tmp_no_hit_df)\n",
    "    tmp_df['q_contig'] = tmp_df['Query'].str.extract(r'([p|h][a-z]*_[^.]*).?')\n",
    "    tmp_df['t_contig'] = tmp_df['Target'].str.extract(r'([p|h][a-z]*_[^.]*).?')\n",
    "    #fix that if you don't extract anything return False and not 'nan'\n",
    "    tmp_df['t_contig'].fillna(False, inplace=True)\n",
    "    tmp_df['q_contig == t_contig'] = (tmp_df[\"Query\"].str.extract(r'[p|h][a-z]*_([0-9]*)') == tmp_df[\"Target\"].str.extract(r'[p|h][a-z]*_([0-9]*)'))\n",
    "    tmp_df.reset_index(inplace=True, drop=True)\n",
    "    blast_out_dict[key] = tmp_df.iloc[:,:]\n",
    "#no make one summary_df for everything.\n",
    "no_filtered_allele_summary_df = pd.concat(blast_out_dict.values())\n",
    "no_filtered_allele_summary_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get all primary contigs with and without haplotigs as pwh_set and pwoh_set\n",
    "p_ctgs = []\n",
    "h_ctgs = []\n",
    "for seq in SeqIO.parse(os.path.join(BASE_A_PATH, p_genome + '.fa'), 'fasta'):\n",
    "    p_ctgs.append(seq.id)\n",
    "for seq in SeqIO.parse(os.path.join(BASE_A_PATH, h_genome + '.fa'), 'fasta'):\n",
    "    h_ctgs.append(seq.id)\n",
    "pwh_set = {re.search(r'[a-z]*_[0-9]*', h_ctg).group().replace('h', 'p') for h_ctg in h_ctgs}\n",
    "pwoh_set = set(pwh_set) - pwh_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add column for being on primary contig\n",
    "no_filtered_allele_summary_df['primary_contig'] = no_filtered_allele_summary_df.q_contig.apply(on_primary_contig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add column for being on primary contig with haplotig\n",
    "no_filtered_allele_summary_df['pwh_contig'] = no_filtered_allele_summary_df.q_contig.apply(pwh_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the list of gene no genome hits \n",
    "no_filtered_allele_gene_no_genome_hit_list = []\n",
    "for key, value in no_filtered_allele_gene_no_genome_hit_dict.items():\n",
    "    no_filtered_allele_gene_no_genome_hit_list += pd.read_csv(value, header=None, sep ='\\t')[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add gene_on_genome hit column\n",
    "no_filtered_allele_summary_df['gene_on_genome_blast_hit'] = ~no_filtered_allele_summary_df[\"Query\"].isin(no_filtered_allele_gene_no_genome_hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good at the exonerate summary step! Please continue\n"
     ]
    }
   ],
   "source": [
    "#quick QC step if exonerate dictonaries and gene_on_genome_blast_hit are the same\n",
    "exn_set = set(list(exonerate_no_filtered_allele_asso_contig_bool_dict.keys()) + list(exonerate_no_filtered_allele_no_asso_contig_bool_dict.keys()))\n",
    "if exn_set == set(no_filtered_allele_summary_df[no_filtered_allele_summary_df.gene_on_genome_blast_hit  == True]['Query'].unique()):\n",
    "    (print(\"All good at the exonerate summary step! Please continue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add exonarate summaries to the summary df\n",
    "no_filtered_allele_summary_df['exn_asso_contig'] = no_filtered_allele_summary_df.Query.apply(exn_asso_contig)\n",
    "no_filtered_allele_summary_df['exn_no_asso_contig'] = no_filtered_allele_summary_df.Query.apply(exn_no_asso_contig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gff_files = [os.path.join(BASE_A_PATH, x) for x in os.listdir(BASE_A_PATH) if x.endswith('anno.gff3')] \n",
    "gff_file_dict ={}\n",
    "for fn in gff_files:\n",
    "    key = fn.split('/')[-1].split('.')[0]\n",
    "    gff_file_dict[key] = fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in gene annotation and gff files for downstream analysis\n",
    "gene_anno_gff_dict = {}\n",
    "for assembly, file in gff_file_dict.items() :\n",
    "    tmp_df =  pd.read_csv(file, header=None, sep='\\t')\n",
    "    tmp_df['protein_id'] = tmp_df[8].apply(col_8_id)\n",
    "    gene_anno_gff_dict[assembly] =  tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop this option for now and simply investigate homozyous regions for all genes e.g. could have gene hit but somewhere else and not at the same position on the associated contig.\n",
    "#filter down the no hits gene vs genome\n",
    "no_gene_vs_genome_hits = \\\n",
    "    no_filtered_allele_summary_df[(no_filtered_allele_summary_df.gene_on_genome_blast_hit == False)]['Query'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get all no filttered allele IDs to investigate if they are in the homozygous region if on p contigs\n",
    "no_filtered_alleles = no_filtered_allele_summary_df['Query'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save out gff files where no_gene_vs_genome blast hits\n",
    "no_gene_vs_genome_gff_dict = {}\n",
    "for assembly, gff in gene_anno_gff_dict.items():\n",
    "    tmp_df_no_gene_vs_genome_df = ''\n",
    "    tmp_df_no_gene_vs_genome_df = gff[gff.protein_id.isin(no_gene_vs_genome_hits)]\n",
    "    out_fn = os.path.join(OUT_PATH_tmp, assembly + '.no_gene_vs_genome.anno.gff')\n",
    "    no_gene_vs_genome_gff_dict[assembly] = out_fn\n",
    "    tmp_df_no_gene_vs_genome_df.iloc[:,0:9].to_csv(out_fn, header=None, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save out gff files for all no filtered alleles\n",
    "no_filtered_alleles_gff_dict = {}\n",
    "for assembly, gff in gene_anno_gff_dict.items():\n",
    "    tmp_df_no_filtered_alleles_df = ''\n",
    "    tmp_df_no_filtered_alleles_df = gff[gff.protein_id.isin(no_filtered_alleles)]\n",
    "    out_fn = os.path.join(OUT_PATH_tmp, assembly + '.Qcov%s.PctID%s.no_filtered_alleles.anno.gff' % (Qcov_cut_off, PctID_cut_off))\n",
    "    no_filtered_alleles_gff_dict[assembly] = out_fn\n",
    "    tmp_df_no_filtered_alleles_df.iloc[:,0:9].to_csv(out_fn, header=None, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the bed dataframe from homozygous coverage of p contigs when mapping againts p and h\n",
    "homo_cov_ph_p_fn = [os.path.join(COV_PATH, x) for x in os.listdir(COV_PATH) if x.endswith(homo_cov_ph_p)][0]\n",
    "homo_cov_ph_p_bed = pybedtools.BedTool(homo_cov_ph_p_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the no gene vs genome df of the primary assembly\n",
    "no_gene_vs_genome_gff_p_bed = pybedtools.BedTool(no_gene_vs_genome_gff_dict[p_genome])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the no gene vs genome df of the primary assembly\n",
    "no_filtered_alleles_gff_p_bed = pybedtools.BedTool(no_filtered_alleles_gff_dict[p_genome])\n",
    "no_filtered_alleles_gff_ph_bed = pybedtools.BedTool(no_filtered_alleles_gff_dict[p_genome.replace('p_ctg', 'ph_ctg')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the id of all genes of no gene vs genome gff\n",
    "gene_ids_ph_p_homo = []\n",
    "for x in no_filtered_alleles_gff_p_bed.intersect(homo_cov_ph_p_bed):\n",
    "    y = col_8_id(x[8])\n",
    "    gene_ids_ph_p_homo.append(y)\n",
    "gene_ids_ph_p_homo = set(gene_ids_ph_p_homo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add a column describing if a protein is encoded in homozygous coverage region on p when mapping against ph\n",
    "no_filtered_allele_summary_df['ph_p_homo_region'] = \\\n",
    "    no_filtered_allele_summary_df.Query.isin(gene_ids_ph_p_homo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#out_fn = os.path.join(OUT_PATH, p_genome.replace('_p_ctg', '_ph_ctg.no_alleles_QC.Qcov%s.PctID%s.df'%(Qcov_cut_off,PctID_cut_off)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#no_filtered_allele_summary_df = pd.read_csv(out_fn, sep='\\t')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "string = ' evm.model.hcontig_000_003.183,evm.model.hcontig_000_003.398,evm.model.hco'\n",
    "pattern = re.compile(r'evm[^,]*')\n",
    "match = re.findall(pattern, string)\n",
    "match = re.findall(pattern, string)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#now read in the proteinorthofile and pull out all the non_singeltons\n",
    "no_singeltons = []\n",
    "pattern = re.compile(r'evm[^,\\t]*')\n",
    "#read one line at a time and patten match evm incluing everything till the next ',' or '\\t'\n",
    "with open(proteinortho_fn, 'r') as proteinortho_fh:\n",
    "    for line in proteinortho_fh:\n",
    "        match = re.findall(pattern, line.rstrip())\n",
    "        for x in match:\n",
    "            no_singeltons.append(x)\n",
    "print(len(no_singeltons))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull in the graph file\n",
    "po_graph_header = ['Target', 'Query', 'evalue_ab', 'bitscore_ab', 'evalue_ba', 'bitscore_ba']\n",
    "po_graph_df = pd.read_csv(proteinortho_graph_fn, sep='\\t',\\\n",
    "                          header=None, comment='#', names=po_graph_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the redundante not signletons\n",
    "no_singeltons = po_graph_df.Target.tolist() + po_graph_df.Query.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_filtered_allele_summary_df['singeltons'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_singeltons_index = no_filtered_allele_summary_df\\\n",
    "    [no_filtered_allele_summary_df.Query.isin(no_singeltons)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_filtered_allele_summary_df.loc[no_singeltons_index, 'singeltons'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of singeltons obtain 7692\n"
     ]
    }
   ],
   "source": [
    "print('This is the number of singeltons obtain %i'% len(no_filtered_allele_summary_df[\\\n",
    "            (no_filtered_allele_summary_df.singeltons == True)]['Query'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the summary dataframe\n",
    "out_fn = os.path.join(OUT_PATH, p_genome.replace('_p_ctg', '_ph_ctg.no_allele_QC.Qcov%s.PctID%s.df'%(Qcov_cut_off,PctID_cut_off)))\n",
    "no_filtered_allele_summary_df.to_csv(out_fn, sep='\\t', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now pull in the vcf files for filtering consider extending this to include multiple SNP callers\n",
    "#dict\n",
    "srm_vcf_fn = [os.path.join(VCF_SRM_PATH, x) for x in os.listdir(VCF_SRM_PATH) if x.endswith(vcf_file_endings)] \n",
    "srm_vcf_dict ={}\n",
    "for fn in srm_vcf_fn:\n",
    "    key_list = fn.split('/')[-1].split('.')\n",
    "    key = '%s.%s' % (key_list[0], key_list[-3])\n",
    "    srm_vcf_dict[key] = fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/pybedtools/bedtool.py:3287: UserWarning: Default names for filetype gff are:\n",
      "['seqname', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attributes']\n",
      "but file has 20 fields; you can supply custom names with the `names` kwarg\n",
      "  % (self.file_type, _names, self.field_count()))\n"
     ]
    }
   ],
   "source": [
    "#now loop through all vcf snp calls and report if a no_filtered_alleles overlaps with SNP when mapping against ph\n",
    "srm_vcf_ph_mapping_keys = [x for x in srm_vcf_dict.keys() if 'ph_ctg' in x]\n",
    "for key in srm_vcf_ph_mapping_keys:\n",
    "    tmp_vcf_bed = pybedtools.BedTool(srm_vcf_dict[key])\n",
    "    #now get the intersect between vcf files and no_filtered_alleles\n",
    "    tmp_gene_ids = []\n",
    "    tmp_df = no_filtered_alleles_gff_ph_bed.intersect(tmp_vcf_bed, wo=True).to_dataframe()\n",
    "    tmp_df['protein_id'] = tmp_df[8].apply(col_8_id)\n",
    "    #get the number of SNPs per coding sequence\n",
    "    tmp_exons_SNP = tmp_df[tmp_df[2] == 'CDS'].groupby('protein_id')[19].sum()\n",
    "    tmp_SNP_dict = dict(zip(tmp_exons_SNP.index, tmp_exons_SNP))\n",
    "    #add the SNP boolean values to the df\n",
    "    column_name = key + '_SNP'\n",
    "    no_filtered_allele_summary_df[column_name] = no_filtered_allele_summary_df.Query.isin(tmp_exons_SNP.index)\n",
    "    #add the #number of SNPs to the df\n",
    "    column_name = column_name +'_#'\n",
    "    no_filtered_allele_summary_df[column_name] = no_filtered_allele_summary_df.apply(lambda row: number_of_SNPs(row['Query'], tmp_SNP_dict), axis =1)\n",
    "    #add the % SNPs per bp in CDS to dataframe\n",
    "    no_filtered_allele_summary_df[column_name.replace('_#', '_%')] = round(no_filtered_allele_summary_df[column_name]*100/(no_filtered_allele_summary_df.QLgth*3+3), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now pull together the *.h_by_p_cov.gff and *.p_by_h_cov.gff files from the allele analysis and save them as \n",
    "#single gff for the whole genome and split out for each haploid genome\n",
    "h_by_p_cov_files = glob.glob(os.path.join(BLAST_RESULT_PATH, '*php_8kbp/*.h_by_p_cov.gff' ))\n",
    "p_by_h_cov_files = glob.glob(os.path.join(BLAST_RESULT_PATH, '*php_8kbp/*.p_by_h_cov.gff' ))\n",
    "p_on_h_cov_files = h_by_p_cov_files + p_by_h_cov_files\n",
    "out_name = os.path.join(BLAST_RESULT_PATH, p_genome.replace('p_ctg','ph_ctg') +'p_on_h_cov.gff')\n",
    "with open(out_name, 'wb') as outfile:\n",
    "    for f in p_on_h_cov_files:\n",
    "        with open(f, 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "tmp_df = pd.read_csv(out_name,sep='\\t', header =None)\n",
    "#for now a fix here for swapping column 3 and 4 of GFF if 4 < 3\n",
    "#tmp_df['comp'] = tmp_df[3] - tmp_df[4]\n",
    "#tmp_swap_index  = tmp_df['comp'] > 0\n",
    "#tmp_df.loc[tmp_swap_index, 3] , tmp_df.loc[tmp_swap_index, 4] = tmp_df[4], tmp_df[3]\n",
    "#tmp_df.sort_values(by=[0,3], inplace=True)\n",
    "#and if 3 == g\n",
    "#is_null_index  = tmp_df[3] == 0\n",
    "#tmp_df.loc[is_null_index, 3] = 1\n",
    "#tmp_df = tmp_df.iloc[:, 0:9]\n",
    "tmp_df[tmp_df[1].str.contains('pcontig')].to_csv(os.path.join(BLAST_RESULT_PATH, h_genome +'.h_by_p_cov.gff'), sep='\\t', header=None, index=None)\n",
    "tmp_df[tmp_df[1].str.contains('hcontig')].to_csv(os.path.join(BLAST_RESULT_PATH, p_genome +'.p_by_h_cov.gff'), sep='\\t', header=None, index=None)\n",
    "tmp_df.to_csv(out_name, sep='\\t', header=None, index=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now add a overlap column to the summary dataframe\n",
    "haplotig_mapping_bed = pybedtools.BedTool(out_name)\n",
    "gene_ids_haplotig_mapping = []\n",
    "for x in no_filtered_alleles_gff_p_bed.intersect(haplotig_mapping_bed):\n",
    "    y = col_8_id(x[8])\n",
    "    gene_ids_haplotig_mapping.append(y)\n",
    "gene_ids_haplotig_mapping = set(gene_ids_haplotig_mapping)\n",
    "no_filtered_allele_summary_df['overlap_p_on_h_mapping'] = \\\n",
    "    no_filtered_allele_summary_df.Query.isin(gene_ids_haplotig_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the summary dataframe\n",
    "out_fn = os.path.join(OUT_PATH, p_genome.replace('_p_ctg', '_ph_ctg.no_alleles_QC.Qcov%s.PctID%s.df'%(Qcov_cut_off,PctID_cut_off)))\n",
    "no_filtered_allele_summary_df.to_csv(out_fn, sep='\\t', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "894"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now write out a couple of ways\n",
    "#no blastp, no exonerate, no homozygous region, no SNPs\n",
    "df = no_filtered_allele_summary_df\n",
    "out_filter = (df.Target == 'False') & (df.exn_asso_contig != True) & (df.exn_no_asso_contig != True) \\\n",
    "    & (df.ph_p_homo_region != True) & (df['Pst_E104_v1_ph_ctg.freebayes_SNP'] != True)\n",
    "out_name = os.path.join(OUT_PATH, p_genome.replace('p_ctg', 'ph_ctg')+'.no_alleles_postQC.txt')\n",
    "np.savetxt(out_name, df[out_filter][\"Query\"].unique(), fmt='%s')\n",
    "len(df[out_filter][\"Query\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no exonerate, homozygous region, SNPs\n",
    "out_filter = (df.exn_asso_contig != True) & (df.exn_no_asso_contig != True) \\\n",
    "    & (df.ph_p_homo_region == True) & (df['Pst_E104_v1_ph_ctg.freebayes_SNP'] == True) #& (df.overlap_p_on_h_mapping == True)\n",
    "out_name = os.path.join(OUT_PATH, p_genome.replace('p_ctg', 'ph_ctg')+'.no_alleles_unphased.txt')\n",
    "np.savetxt(out_name, df[out_filter][\"Query\"].unique(), fmt='%s')\n",
    "len(df[out_filter][\"Query\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no exonerate, no homozygous region, no SNPs, overlap p on h mapping\n",
    "out_filter =  (df.ph_p_homo_region == True) & (df['Pst_E104_v1_ph_ctg.freebayes_SNP'] == True) #& (df.overlap_p_on_h_mapping == True)\n",
    "#out_name = os.path.join(OUT_PATH, p_genome.replace('p_ctg', 'ph_ctg')+'.no_alleles_unphased.txt')\n",
    "#np.savetxt(out_name, df[out_filter][\"Query\"].unique(), fmt='%s')\n",
    "len(df[out_filter][\"Query\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if clean_up == True:\n",
    "    shutil.rmtree(OUT_PATH_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
